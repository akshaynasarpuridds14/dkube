{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "import kfp\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "components_url = \"/mnt/dkube/pipeline/components/\"\n",
    "dkube_preprocessing_op   = kfp.components.load_component_from_file(components_url + \"preprocess/component.yaml\")\n",
    "dkube_training_op        = kfp.components.load_component_from_file(components_url + \"training/component.yaml\")\n",
    "dkube_serving_op         = kfp.components.load_component_from_file(components_url + \"serving/component.yaml\")\n",
    "token = os.getenv(\"DKUBE_USER_ACCESS_TOKEN\")\n",
    "client = kfp.Client(host=os.getenv(\"KF_PIPELINES_ENDPOINT\"), existing_token=token, namespace=os.getenv(\"USERNAME\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_script = f\"pip install py7zr --user && python cifar10preprocessing.py\"\n",
    "input_mount_point = \"/opt/dkube/input\"\n",
    "output_mount_point = \"/opt/dkube/output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "@kfp.dsl.pipeline(\n",
    "    name='dkube-mnist-pl',\n",
    "    description='sample mnist pipeline with dkube components'\n",
    ")\n",
    "def cifar10_pipeline(program='cifar10', dataset_preprocess='cifar10', dataset='cifar10-extracted', model='cifar10'):\n",
    "#     print(dataset_preprocess)\n",
    "    \n",
    "    preprocessing = dkube_preprocessing_op(auth_token = token,container=json.dumps({\"image\": \"ocdr/dkube-datascience-tf-cpu:v2.0.0\"}),\n",
    "                                           program=str(program), run_script=preprocessing_script,\n",
    "                                           datasets=json.dumps([str(dataset_preprocess)]), \n",
    "                                           outputs=json.dumps([str(dataset)]),\n",
    "                                           input_dataset_mounts=json.dumps([input_mount_point]), \n",
    "                                           output_mounts=json.dumps([output_mount_point])\n",
    "                                            )\n",
    "\n",
    "    train        = dkube_training_op(container='{\"image\":\"ocdr/dkube-datascience-tf-cpu:v2.0.0\"}',\n",
    "                                    framework=\"tensorflow\", version=\"2.0.0\",\n",
    "                                    program=str(program), run_script=\"python cifar10training.py\",\n",
    "                                    datasets=json.dumps([str(dataset)]), outputs=json.dumps([str(model)]),\n",
    "                                    input_dataset_mounts=json.dumps([input_mount_point]),\n",
    "#                                     input_dataset_mounts='[\"/cifar10\"]',\n",
    "                                    output_mounts=json.dumps([output_mount_point]),\n",
    "#                                     output_mounts='[\"/model\"]',\n",
    "                                    envs='[{\"EPOCHS\": \"1\"}]',\n",
    "                                    auth_token=token).after(preprocessing)\n",
    "\n",
    "    serving     = dkube_serving_op(model=train.outputs['artifact'], device='cpu', \n",
    "                                    serving_image='{\"image\":\"ocdr/tensorflowserver:2.0.0\"}',\n",
    "                                    auth_token=token).after(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#     serving     = dkube_serving_op(model=train.outputs['artifact'], device='cpu', \n",
    "#                                     serving_image='{\"image\":\"ocdr/tensorflowserver:2.0.0\"}',\n",
    "#                                     transformer_image='{\"image\":\"ocdr/dkube-datascience-tf-cpu:v2.0.0\"}',\n",
    "#                                     transformer_project=str(program),\n",
    "#                                     transformer_code='mnist/transformer.py', auth_token=token).after(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Experiment link <a href=\"/pipeline/#/experiments/details/ae6c3e54-5cc6-4dd9-b2b5-87d475fb1306\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run link <a href=\"/pipeline/#/runs/details/312b0642-8779-466b-bc98-69706942e1d9\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "RunPipelineResult(run_id=312b0642-8779-466b-bc98-69706942e1d9)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.create_run_from_pipeline_func(cifar10_pipeline, arguments={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate & upload pipeline (Optional)\n",
    "import kfp.compiler as compiler\n",
    "compiler.Compiler().compile(cifar10_pipeline, \"cifar10-pipeline.zip\")\n",
    "client.upload_pipeline(\"cifar10-pipeline.zip\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
